{
    "content": [
        {
            "type": "title",
            "text": "Distribuzioni di probabilità: variabili continue"
        },
        {
            "type": "subtitle",
            "text": "In questo capitolo esploriamo le distribuzioni di probabilità per variabili continue, la loro densità di probabilità e gli indicatori statistici associati. Inoltre, approfondiremo la distribuzione gaussiana e le sue applicazioni."
        },
        {
            "type": "heading",
            "level": 3,
            "text": "Densità di probabilità per variabili continue"
        },
        {
            "type": "paragraph",
            "text": "Siamo ora nel caso in cui la variabile casuale può assumere un numero infinito di valori in un intervallo continuo, dobbiamo perciò introdurre il concetto di <highlight>densità di probabilità</highlight>:"
        },
        {
            "type": "math",
            "text": "P(x_0 \\leq x \\leq x_0 + dx) = p(x_0) dx"
        },
        {
            "type": "paragraph",
            "text": "Dove \\(P(x_0 \\leq x \\leq x_0 + dx)\\) è la probabilità che la variabile \\(x\\) assuma un valore compreso tra \\(x_0\\) e \\(x_0 + dx\\), e \\(p(x_0)\\) è la densità di probabilità in quel punto. Inoltre, si noti che:"
        },
        {
            "type": "list",
            "items": [
                "La densità di probabilità ha le dimensioni del reciproco della variabile \\(x\\) (infatti la probabilità \\(P\\) è adimensionale).",
                "La probabilità \\(P\\) dipende dalle dimensioni dell'intervallo \\(dx\\), mentre la densità di probabilità \\(p(x)\\) no."
            ]
        },
        {
            "type": "paragraph",
            "text": "Sfrutteremo la densità di probabilità per ricavare la <highlight>funzione della densità di probabilità</highlight>>."
        },
        {
            "type": "image",
            "src": "/anno-1/el-fisica-con-statistica/img/densita-probabilita-variabile-continua.png",
            "alt": "densita-probabilita-variabile-continua"
        },
        {
            "type": "paragraph",
            "text": "Da notare che, anche se la funzione densità di probabilità ha un valore preciso per un certo \\(x_0)\\, la probabilità che la variabile assuma esattamente quel valore è zero perchè l'area sottesa alla funzione è nulla."
        },
        {
            "type": "paragraph",
            "text": "La probabilità in un intervallo finito è quindi data da:"
        },
        {
            "type": "math",
            "text": "P(a \\leq x \\leq b) = \\int_{a}^{b} p(x) dx"
        },
        {
            "type": "paragraph",
            "text": "La funzione densità di probabilità deve inoltre essere normalizzata, cioè l'integrale su tutto il dominio della variabile deve essere uguale a 1:"
        },
        {
            "type": "math",
            "text": "\\int_{-\\infty}^{\\infty} p(x) dx = 1"
        },
        {
            "type": "paragraph",
            "text": "Attenzione comunque al fatto che se la variabile è definita solo in un intervallo, allora \\(p(x) = 0\\) al di fuori di quell'intervallo e l'integrale di normalizzazione resta comunque definito e finito."
        },
        {
            "type": "heading",
            "level": 4,
            "text": "Indicatori statistici per una distribuzione di probabilità continua"
        },
        {
            "type": "paragraph",
            "text": "Il <highlight>valore di aspettazione</highlight> per una variabile continua si calcola come:"
        },
        {
            "type": "math",
            "text": "< x > = \\int_{-\\infty}^{\\infty} x \\cdot p(x) dx"
        },
        {
            "type": "paragraph",
            "text": "La <highlight>varianza</highlight> si calcola come:"
        },
        {
            "type": "math",
            "text": "\\sigma_x^2 = \\int_{-\\infty}^{\\infty} (x - < x >)^2 \\cdot p(x) dx = < x^2 > - < x >^2"
        },
        {
            "type": "paragraph",
            "text": "Vediamo velocemente anche il caso della probabilità costante in un intervallo \\([a, b]\\):"
        },
        {
            "type": "math",
            "text": "p(x) = \\begin{cases} C = \\frac{1}{b-a} & \\text{se } a \\leq x \\leq b \\\\ 0 & \\text{altrimenti} \\end{cases}"
        },
        {
            "type": "paragraph",
            "text": "In questo caso, il valore di aspettazione è:"
        },
        {
            "type": "math",
            "text": "< x > = \\int_{a}^{b} x \\cdot \\frac{1}{b-a} dx = \\frac{a + b}{2}"
        },
        {
            "type": "paragraph",
            "text": "La varianza è:"
        },
        {
            "type": "math",
            "text": "\\sigma_x^2 = \\int_{a}^{b} \\left(x - \\frac{a+b}{2}\\right)^2 \\cdot \\frac{1}{b-a} dx = \\frac{(b-a)^2}{12}"
        },
        {
            "type": "heading",
            "level": 3,
            "text": "Distribuzione Gaussiana"
        },
        {
            "type": "paragraph",
            "text": "La <highlight>distribuzione gaussiana</highlight>, o distribuzione normale, è una particolare distribuzione con delle caratteristiche ben definite. Vediamo prima di tutto la sua funzione densità di probabilità:"
        },
        {
            "type": "math",
            "text": "p(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2} \\left( \\frac{x - < x >}{\\sigma} \\right)^2}"
        },
        {
            "type": "paragraph",
            "text": "Anche scritta come:"
        },
        {
            "type": "math",
            "text": "f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2 }} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}"
        },
        {
            "type": "paragraph",
            "text": "dove \\(\\mu = < x >\\) è il valore di aspettazione e \\(\\sigma^2\\) è la varianza della distribuzione."
        },
        {
            "type": "paragraph",
            "text": "Elencando le sue proprietà principali:"
        },
        {
            "type": "list",
            "items": [
                "La distribuzione è simmetrica rispetto al valore di aspettazione \\(< x >\\).",
                "La larghezza della distribuzione è determinata dalla deviazione standard \\(\\sigma\\).",
                "Variando il valore di aspettazione \\(< x >\\), la distribuzione si sposta lungo l'asse delle x senza cambiare forma.",
                "Variando la deviazione standard \\(\\sigma\\), la distribuzione si allarga o si restringe intorno al valore di aspettazione."
            ]
        },
        {
            "type": "paragraph",
            "text": "La distribuzione gaussiana può inoltre, per valori molto grandi di \\(N\\), essere utilizzata per approssimare altre distribuzioni di probabilità, come la distribuzione binomiale e la distribuzione di Poisson, grazie al teorema del limite centrale. <br> Prima di fare ciò ricordiamo che quando si approssima una distribuzione discreta con una continua, è buona norma applicare la <highlight>correzione di continuità</highlight>, che consiste nel considerare l'intervallo \\([k - 0.5, k + 0.5]\\) invece del singolo punto \\(k\\). <br> Nell'atto pratico questo significa che:"
        },
        {
            "type": "math",
            "text": "P(K = k) \\approx P(k - 0.5 \\le X \\le k + 0.5) \\quad \\quad P(K \\le k) \\approx P(X \\le k + 0.5) \\quad \\quad P(K \\ge k) \\approx P(X \\ge k - 0.5)"
        },
        {
            "type": "math",
            "text": "P(K < k) \\approx P(X \\le k - 0.5) \\quad \\quad P(K > k) \\approx P(X \\ge k + 0.5)"
        },
        {
            "type": "paragraph",
            "text": "dove \\(K\\) è la variabile discreta (\\(K \\sim B(n, p)\\) o \\(K \\sim P(\\lambda)\\)) e \\(X \\sim N(< k >, \\sigma_k)\\) è la variabile continua che la approssima."
        },
        {
            "type": "heading",
            "level": 4,
            "text": "Approssimazione della distribuzione binomiale con la gaussiana"
        },
        {
            "type": "paragraph",
            "text": "Per valori grandi di \\(n\\) (circa maggiore di 10) e quando \\(p\\) non è troppo vicino a 0 o 1, la distribuzione binomiale può essere approssimata da una distribuzione gaussiana con media \\(< k > = np\\) e deviazione standard \\(\\sigma_k = \\sqrt{np(1-p)}\\):"
        },
        {
            "type": "math",
            "text": "P(k; n, p) \\approx \\frac{1}{\\sqrt{2 \\pi} \\sqrt{np(1-p)}} e^{-\\frac{1}{2} \\left( \\frac{k - np}{\\sqrt{np(1-p)}} \\right)^2}"
        },
        {
            "type": "heading",
            "level": 4,
            "text": "Approssimazione della distribuzione di Poisson con la gaussiana"
        },
        {
            "type": "paragraph",
            "text": "Per valori grandi di \\(\\lambda\\) (circa maggiore di 10), la distribuzione di Poisson può essere approssimata da una distribuzione gaussiana con media \\(< k > = \\lambda\\) e deviazione standard \\(\\sigma_k = \\sqrt{\\lambda}\\):"
        },
        {
            "type": "math",
            "text": "P(k; \\lambda) \\approx \\frac{1}{\\sqrt{\\lambda} \\sqrt{2 \\pi}} e^{-\\frac{1}{2} \\left( \\frac{k - \\lambda}{\\sqrt{\\lambda}} \\right)^2}"
        },
        {
            "type": "heading",
            "level": 4,
            "text": "Standardizzazione"
        },
        {
            "type": "paragraph",
            "text": "Spesso è utile standardizzare una variabile casuale con media \\(< x >\\) e deviazione standard \\(\\sigma_x\\) per ottenere una nuova variabile casuale \\(Z\\) con media 0 e deviazione standard 1. La standardizzazione si ottiene con la seguente trasformazione:"
        },
        {
            "type": "math",
            "text": "Z = \\frac{X - < x >}{\\sigma_x}"
        },
        {
            "type": "paragraph",
            "text": "Cosi facendo possiamo ottenere una distribuzione che non dipende da alcun paramentro specifico. Nel caso della Gaussiana, la distribuzione standardizzata diventa:"
        },
        {
            "type": "math",
            "text": "p(z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2} z^2}"
        },
        {
            "type": "image",
            "src": "/anno-1/el-fisica-con-statistica/img/gaussiana-standardizzata.png",
            "alt": "gaussiana-standardizzata"
        },
        {
            "type": "paragraph",
            "text": "Utilizzando quindi \\((16)\\) e ricordando \\((15)\\), possiamo calcolare le probabilità per qualsiasi distribuzione gaussiana conoscendo solo la tabella della distribuzione standardizzata."
        },
        {
            "type": "paragraph",
            "text": "Inserire guida alla lettura delle tabelle standardizzate con esercizi etc etc..."
        },
        {
            "type": "heading",
            "level": 3,
            "text": "Teorema del Limite Centrale"
        },
        {
            "type": "paragraph",
            "text": "Il <highlight>Teorema del Limite Centrale</highlight> afferma che, sia \\(\\mathbb{X} = \\displaystyle \\sum x_i\\) una variabile casuale definita come somma di \\(\\mathbb{N}\\) variabili casuali indipendenti e ognuna descritta dalla propria distribuzione qualsiasi, con valori attesi \\(\\mu_i\\) finiti e varianze \\(< x_i - \\mu_i >^2 = \\sigma_i^2\\) finite. <br> Allora, la distribuzione di probabilità di \\(\\mathbb{X}\\):"
        },
        {
            "type": "list",
            "items": [
                "Ha valore atteso \\(< \\mathbb{X} > \\ = \\  \\displaystyle \\sum \\mu_i\\)",
                "Ha varianza \\(\\sigma_{\\mathbb{X}}^2 \\ = \\ \\displaystyle \\sum \\sigma_i^2\\)",
                "Per \\(\\mathbb{N} \\to \\infty\\), la distribuzione di probabilità di \\(\\mathbb{X}\\) tende ad una distribuzione gaussiana."
            ]
        },
        {
            "type": "paragraph",
            "text": "Questo teorema si applica a qualsiasi variabile aleatoria che sia la combinazione lineare di \\(\\mathbb{N}\\) altre variabili aleatorie indipendenti, ognuna con una sua distribuzione specifica, con un \\(< x >\\) finito e una varianza \"confrontabile\". <br> Un esempio sono gli \"errori\" di fabbricazione (le incertezze sugli errori sono la somma di tante cause diverse), il teorema del limite centrale ci dice che, se gli effetti che ogni singolo errore ha sono indipendenti tra loro e nessuno domina, la somma di questi errori ha una distribuzione che tende a essere gaussiana."
        },
        {
            "type": "heading",
            "level": 3,
            "text": "Trattamento delle incertezze/errori"
        },
        {
            "type": "paragraph",
            "text": "Riprendiamo velocemente le due definizioni da cui il titolo:"
        },
        {
            "type": "list",
            "items": [
                "<highlight>Errore</highlight>: differenza tra il valore misurato e il valore vero della quantità misurata. Può essere sistematico o casuale.",
                "<highlight>Incertezza</highlight>: stima della dispersione dei valori misurati intorno al valore vero. Rappresenta la precisione della misura."
            ]
        },
        {
            "type": "heading",
            "level": 4,
            "text": "media come miglior stima del valore vero"
        },
        {
            "type": "paragraph",
            "text": "Quando si effettuano più misure di una stessa quantità, la media aritmetica delle misure è considerata la miglior stima del valore vero della quantità misurata. Iniziamo prendendo la media come:"
        },
        {
            "type": "math",
            "text": "\\bar{x} = \\frac{x_1 + x_2 + ... + x_N}{N} = \\frac{1}{N} \\displaystyle \\sum_{i=1}^{N} x_i"
        },
        {
            "type": "paragraph",
            "text": "Ora, sapendo che il valore atteso di ogni singola misura è \\(\\mu_i = < x_i >\\), e che, poichè le misure sono indipendenti e la distribuzione è identica per ogni osservazione, allora \\(\\mu_i = \\mu\\), di conseguenza:"
        },
        {
            "type": "math",
            "text": "< \\bar{x} > \\ = \\ \\frac{1}{N} \\displaystyle \\sum_{i=1}^{N} < x_i > \\ = \\ \\frac{1}{N} \\displaystyle \\sum_{i=1}^{N} \\mu_i \\ = \\ \\frac{1}{N} \\cdot N \\mu \\ = \\ \\mu \\ = \\ < x > \\quad \\text{e} \\quad \\sigma_{\\bar{x}}^2 \\ = \\ \\frac{\\sigma^2}{N}"
        }
    ]
}